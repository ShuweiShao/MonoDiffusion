<div align="center">

<h1>MonoDiffusion: Self-Supervised Monocular Depth Estimation Using Diffusion Model</h1>

<div>
    <a href='https://scholar.google.com.hk/citations?hl=zh-CN&user=ecZHSVQAAAAJ' target='_blank'>Shuwei Shao</a><sup>1</sup>&emsp;
    <a target='_blank'>Zhongcai Pei</a><sup>1</sup>&emsp;
    <a href='https://scholar.google.com.hk/citations?hl=zh-CN&user=5PoZrcYAAAAJ' target='_blank'>Weihai Chen</a><sup>1</sup>&emsp;
    <a target='_blank'>Dingchi Sun</a><sup>1</sup>&emsp;
    <a href='https://scholar.google.com.hk/citations?hl=zh-CN&user=7E0QgKUAAAAJ' target='_blank'>Peter C. Y. Chen</a><sup>2</sup>&emsp;
    <a href='https://scholar.google.com.hk/citations?hl=zh-CN&user=LiUX7WQAAAAJ' target='_blank'>Zhengguo Li</a><sup>3</sup>&emsp;
</div>
<div>
    <sup>1</sup>Beihang University, <sup>2</sup>National University of Singapore, <sup>3</sup>A*STAR
</div>


<div>
    <h4 align="center">
        ‚Ä¢ <a href="https://ieeexplore.ieee.org/abstract/document/10772032" target='_blank'>TCSVT 2024</a> ‚Ä¢
    </h4>
</div>

## Abstract

<div style="text-align:center">
<img src="assets/teaser.jpg"  width="80%" height="80%">
</div>

</div>
<strong>We introduce a novel self-supervised depth estimation framework, dubbed MonoDiffusion, by formulating it as an iterative denoising process. Because the depth ground-truth is unavailable in the training phase, we develop a pseudo ground-truth diffusion process to assist the diffusion in MonoDiffusion. The pseudo ground-truth diffusion gradually adds noise to the depth map generated by a pre-trained teacher model. Moreover, the teacher model allows applying a distillation loss to guide the denoised depth. Further, we develop a masked visual condition mechanism to enhance the denoising ability of model. Extensive experiments are conducted on the KITTI and Make3D datasets and the proposed MonoDiffusion outperforms prior state-of-the-art competitors.<strong> 


## üíæ Overview

<p align="center">
<img src='assets/overview.jpg' width=800/> 
</p>


## ‚öôÔ∏è Setup

The experimental environments and command setup are based on [Lite-Mono](https://github.com/noahzn/Lite-Mono).

## üì¶ Model zoo

You can download the model weights from the following [link](https://drive.google.com/file/d/1pVslFg2cnAtMrv2YP2qKGNm9JppibIBi/view?usp=sharing).

## ‚úèÔ∏è üìÑ Citation

If you find our work useful in your research please consider citing our paper:

```
@article{shao2024monodiffusion,
  title={MonoDiffusion: self-supervised monocular depth estimation using diffusion model},
  author={Shao, Shuwei and Pei, Zhongcai and Chen, Weihai and Sun, Dingchi and Chen, Peter CY and Li, Zhengguo},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  year={2024},
  publisher={IEEE}
}
```

## Acknowledgement

Our code is based on the implementation of [Lite-Mono](https://github.com/noahzn/Lite-Mono). We thank the authors for their excellent work and repository.

